{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation on ZeroSpeech'17 English CPC-Big Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Herman Kamper, MIT License\n",
    "\n",
    "Train a segmental autoencoding recurrent neural network (segmental AE-RNN) and perform word segmentation on encoded ZeroSpeech'17 English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.stats import gamma\n",
    "from sklearn import cluster\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from seg_aernn import datasets, models, viterbi\n",
    "from utils import eval_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmented_sentence(ids, boundaries):\n",
    "    output = \"\"\n",
    "    cur_word = []\n",
    "    for i_symbol, boundary in enumerate(boundaries):\n",
    "        cur_word.append(id_to_symbol[ids[i_symbol]])\n",
    "        if boundary:\n",
    "            output += \"_\".join(cur_word)\n",
    "            output += \" \"\n",
    "            cur_word = []\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration penalty functions\n",
    "\n",
    "\"\"\"\n",
    "# Histogram\n",
    "histogram = np.array([\n",
    "    0, 1.66322800e-01, 2.35838129e-01, 2.10609187e-01,\n",
    "    1.48025482e-01, 9.42918160e-02, 5.84211098e-02, 3.64679480e-02,\n",
    "    2.18264741e-02, 1.25420784e-02, 7.18500018e-03, 4.27118399e-03,\n",
    "    1.73743077e-03, 1.19448366e-03, 7.42027726e-04, 2.89571796e-04,\n",
    "    2.35277084e-04, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001\n",
    "    ])  # to-do: check this\n",
    "histogram = histogram/np.sum(histogram)\n",
    "def neg_log_hist(dur):\n",
    "    return -np.log(0 if dur >= len(histogram) else histogram[dur])\n",
    "\n",
    "# Cached Gamma\n",
    "# shape, loc, scale = (2.3, 0, 1.3)  # VQ-VAE\n",
    "shape, loc, scale = (2.6, 0, 1.8)    # CPC-big\n",
    "# shape, loc, scale = (2.5, 0, 1.5)    # CPC-big (Gamma)\n",
    "gamma_cache = []\n",
    "for dur in range(200):\n",
    "    gamma_cache.append(gamma.pdf(dur, shape, loc, scale))\n",
    "gamma_cache = np.array(gamma_cache)/np.sum(gamma_cache)\n",
    "def neg_log_gamma(dur):\n",
    "    if dur < 200:\n",
    "        return -np.log(gamma_cache[dur])\n",
    "    else:\n",
    "        return -np.log(0)\n",
    "\"\"\"\n",
    "    \n",
    "# Chorowski\n",
    "def neg_chorowski(dur):\n",
    "    return -(dur - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "vq_model = \"cpc_big\"\n",
    "dataset = \"zs2017_en\"\n",
    "split = \"train\"\n",
    "seg_tag = \"phoneseg_dp_penalized\"\n",
    "\n",
    "# Paths\n",
    "seg_dir = (\n",
    "    Path(\"../../vqwordseg/exp\")/vq_model/dataset/split/seg_tag/\"intervals\"\n",
    "    )\n",
    "# word_ref_dir = Path(\"../../vqwordseg/data\")/dataset/\"word_intervals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ../../vqwordseg/exp/cpc_big/zs2017_en/train/phoneseg_dp_penalized/intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74785/74785 [22:19<00:00, 55.84it/s] \n"
     ]
    }
   ],
   "source": [
    "# Read phone segmentation\n",
    "phoneseg_interval_dict = {}\n",
    "print(\"Reading: {}\".format(seg_dir))\n",
    "phoneseg_interval_dict = eval_segmentation.get_intervals_from_dir(seg_dir)\n",
    "utterances = phoneseg_interval_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp\n",
    "# utterances = list(utterances)[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Merge repeated codes (only possible for intervals > 15 frames)\\nmerged_dict = {}\\nfor utt_key in tqdm(phoneseg_interval_dict):\\n    i_token = 0\\n    while i_token < len(phoneseg_interval_dict[utt_key]) - 1:\\n        cur_start, cur_end, cur_label = phoneseg_interval_dict[utt_key][i_token]\\n        next_start, next_end, next_label = phoneseg_interval_dict[utt_key][i_token + 1]\\n        if cur_label == next_label:\\n            phoneseg_interval_dict[utt_key].pop(i_token)\\n            phoneseg_interval_dict[utt_key].pop(i_token)\\n            phoneseg_interval_dict[utt_key].insert(\\n                i_token,\\n                (cur_start, next_end, cur_label)\\n                )\\n        else:\\n            i_token += 1\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Merge repeated codes (only possible for intervals > 15 frames)\n",
    "merged_dict = {}\n",
    "for utt_key in tqdm(phoneseg_interval_dict):\n",
    "    i_token = 0\n",
    "    while i_token < len(phoneseg_interval_dict[utt_key]) - 1:\n",
    "        cur_start, cur_end, cur_label = phoneseg_interval_dict[utt_key][i_token]\n",
    "        next_start, next_end, next_label = phoneseg_interval_dict[utt_key][i_token + 1]\n",
    "        if cur_label == next_label:\n",
    "            phoneseg_interval_dict[utt_key].pop(i_token)\n",
    "            phoneseg_interval_dict[utt_key].pop(i_token)\n",
    "            phoneseg_interval_dict[utt_key].insert(\n",
    "                i_token,\n",
    "                (cur_start, next_end, cur_label)\n",
    "                )\n",
    "        else:\n",
    "            i_token += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74785/74785 [00:00<00:00, 463659.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28_26_5_11_2_23_1_34_15_26_5_27_23_17_5_40_18_20_6_33_44_0_25_40_38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_text = []\n",
    "# n_max_sentence_length = 800\n",
    "for utt_key in tqdm(utterances):\n",
    "    prepared_text.append(\n",
    "        \"_\".join([i[2] for i in phoneseg_interval_dict[utt_key]])\n",
    "#         \"_\".join([i[2] for i in phoneseg_interval_dict[utt_key]][:n_max_sentence_length])\n",
    "        )\n",
    "    \n",
    "print(prepared_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 23, 49, 7, 16, 20, 5, 32, 11, 23, 49, 24, 20, 13, 49, 39, 14, 17, 50, 31, 43, 4, 22, 39, 36]\n",
      "['28', '26', '5', '11', '2', '23', '1', '34', '15', '26', '5', '27', '23', '17', '5', '40', '18', '20', '6', '33', '44', '0', '25', '40', '38']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "PAD_SYMBOL      = \"<pad>\"\n",
    "SOS_SYMBOL      = \"<s>\"    # start of sentence\n",
    "EOS_SYMBOL      = \"</s>\"   # end of sentence\n",
    "BOUNDARY_SYMBOL = \" \"      # word boundary\n",
    "symbols = set()\n",
    "for sentence in prepared_text:\n",
    "    for char in sentence.split(\"_\"):\n",
    "        symbols.add(char)\n",
    "SYMBOLS = [PAD_SYMBOL, SOS_SYMBOL, EOS_SYMBOL, BOUNDARY_SYMBOL] + (sorted(list(symbols)))\n",
    "symbol_to_id = {s: i for i, s in enumerate(SYMBOLS)}\n",
    "id_to_symbol = {i: s for i, s in enumerate(SYMBOLS)}\n",
    "\n",
    "def text_to_id(text, add_sos_eos=False):\n",
    "    \"\"\"\n",
    "    Convert text to a list of symbol IDs.\n",
    "\n",
    "    Sentence start and end symbols can be added by setting `add_sos_eos`.\n",
    "    \"\"\"\n",
    "    symbol_ids = []\n",
    "    for word in text.split(\" \"):\n",
    "        for code in word.split(\"_\"):\n",
    "            symbol_ids.append(symbol_to_id[code])\n",
    "        symbol_ids.append(symbol_to_id[BOUNDARY_SYMBOL])\n",
    "    symbol_ids = symbol_ids[:-1]  # remove last space\n",
    "\n",
    "    if add_sos_eos:\n",
    "        return [symbol_to_id[SOS_SYMBOL]] + symbol_ids + [symbol_to_id[EOS_SYMBOL]]\n",
    "    else:\n",
    "        return symbol_ids\n",
    "\n",
    "print(text_to_id(prepared_text[0]))\n",
    "print([id_to_symbol[i] for i in text_to_id(prepared_text[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28_26_5_11_2_23_1_34_15_26_5_27_23_17_5_40_18_20_6_33_44_0_25_40_38\n",
      "28_14_26_30_27_43_44_32_14_10_42_17_33_43_28_45_14_10_17_26_41_47_15_9_44_45_43_47_24_29_18_6_42_33_10_11_2_23_47_24_29_42_14_1_10_27_3_40_21_45_37_18_20_22_15_9_38\n",
      "13\n",
      "27_23_47_26_41_42_43\n",
      "13_36_39_14_1_34_26_5_25_10_42_37_18_20_22_9_38\n",
      "13_36_39_37_22_10_44_14_10_47_24_29_46_2_23_10_17_30_38\n",
      "28_44_32_23_37_18_20_6_27_23_27_23_38\n"
     ]
    }
   ],
   "source": [
    "# First three words of training data\n",
    "word_dataset = datasets.WordDataset(prepared_text, text_to_id)\n",
    "for i in range(7):\n",
    "    sample = word_dataset[i]\n",
    "    print(\"_\".join([id_to_symbol[i] for i in sample.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. train sentences: 74785\n",
      "Examples: ['28_26_5_11_2_23_1_34_15_26_5_27_23_17_5_40_18_20_6_33_44_0_25_40_38', '28_14_26_30_27_43_44_32_14_10_42_17_33_43_28_45_14_10_17_26_41_47_15_9_44_45_43_47_24_29_18_6_42_33_10_11_2_23_47_24_29_42_14_1_10_27_3_40_21_45_37_18_20_22_15_9_38', '13']\n",
      "Min length:  1\n",
      "Max length:  254\n",
      "Mean length: 31.6870\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "cur_val_sentences = prepared_text[-1000:]\n",
    "cur_train_sentences = prepared_text\n",
    "# cur_train_sentences = prepared_text[:15000]\n",
    "\n",
    "# Random boundaries\n",
    "np.random.seed(42)\n",
    "# cur_train_sentences = insert_random_boundaries(cur_train_sentences)\n",
    "# cur_val_sentences = insert_random_boundaries(cur_val_sentences)\n",
    "\n",
    "print(\"No. train sentences:\", len(cur_train_sentences))\n",
    "print(\"Examples:\", cur_train_sentences[:3])\n",
    "print(\"Min length: \", min([len(i.split(\"_\")) for i in cur_train_sentences]))\n",
    "print(\"Max length: \", max([len(i.split(\"_\")) for i in cur_train_sentences]))\n",
    "print(\"Mean length: {:.4f}\".format(np.mean([len(i.split(\"_\")) for i in cur_train_sentences])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(i.split(\"_\")) for i in cur_train_sentences], 100, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE-RNN model\n",
    "n_symbols = len(SYMBOLS)\n",
    "symbol_embedding_dim = 10  # 25\n",
    "hidden_dim = 500  # 250  # 500  # 1000  # 200\n",
    "embedding_dim = 50  # 150  # 300  # 25\n",
    "teacher_forcing_ratio = 0.5  # 1.0  # 0.5  # 1.0\n",
    "n_encoder_layers = 1  # 1  # 3  # 10\n",
    "n_decoder_layers = 1  # 1  # 1\n",
    "batch_size = 32  # 32*3  # 32\n",
    "learning_rate = 0.001  # 0.001\n",
    "input_dropout = 0.0  # 0.0 # 0.5\n",
    "dropout = 0.0\n",
    "n_symbols_max = 50  # 25\n",
    "# n_epochs_max = 5\n",
    "n_epochs_max = None  # determined from n_max_steps and batch size\n",
    "n_steps_max = 1500  # 1500  # 2500  # 1000  # None\n",
    "# n_steps_max = None  # Only use n_epochs_max\n",
    "bidirectional_encoder = False  # False\n",
    "\n",
    "encoder = models.Encoder(\n",
    "    n_symbols=n_symbols,\n",
    "    symbol_embedding_dim=symbol_embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers=n_encoder_layers,\n",
    "    dropout=dropout,\n",
    "    input_dropout=input_dropout,\n",
    "    bidirectional=bidirectional_encoder\n",
    "    )\n",
    "decoder = models.Decoder1(\n",
    "    n_symbols=n_symbols,\n",
    "    symbol_embedding_dim=symbol_embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers=n_decoder_layers,\n",
    "    sos_id = symbol_to_id[SOS_SYMBOL],\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "    dropout=dropout\n",
    "    )\n",
    "# decoder = models.Decoder2(\n",
    "#     n_symbols=n_symbols,\n",
    "#     hidden_dim=hidden_dim,\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     n_layers=n_decoder_layers,\n",
    "#     dropout=dropout\n",
    "#     )\n",
    "model = models.EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2338/2338 [01:30<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 72.230, val loss: 73.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 161/2338 [00:06<01:21, 26.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 61.438, val loss: 70.128\n"
     ]
    }
   ],
   "source": [
    "# Training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Training data\n",
    "train_dataset = datasets.WordDataset(\n",
    "    cur_train_sentences, text_to_id, n_symbols_max=n_symbols_max\n",
    "    )\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=datasets.pad_collate\n",
    "    )\n",
    "\n",
    "# Validation data\n",
    "val_dataset = datasets.WordDataset(cur_val_sentences, text_to_id)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=datasets.pad_collate\n",
    "    )\n",
    "\n",
    "# Loss\n",
    "criterion = nn.NLLLoss(\n",
    "    reduction=\"sum\", ignore_index=symbol_to_id[PAD_SYMBOL]\n",
    "    )\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if n_epochs_max is None:\n",
    "    steps_per_epoch = np.ceil(len(cur_train_sentences)/batch_size)\n",
    "    n_epochs_max = int(np.ceil(n_steps_max/steps_per_epoch))\n",
    "\n",
    "i_step = 0\n",
    "for i_epoch in range(n_epochs_max):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(train_loader)):\n",
    "        optimiser.zero_grad()\n",
    "        data = data.to(device)       \n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_output.contiguous().view(-1, decoder_output.size(-1)),\n",
    "            data.contiguous().view(-1)\n",
    "            )\n",
    "        loss /= len(data_lengths)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_losses.append(loss.item())\n",
    "        i_step += 1\n",
    "        if i_step == n_steps_max and n_steps_max is not None:\n",
    "            break\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (data, data_lengths) in enumerate(val_loader):\n",
    "            data = data.to(device)            \n",
    "            encoder_embedding, decoder_output = model(\n",
    "                data, data_lengths, data, data_lengths\n",
    "                )\n",
    "\n",
    "            loss = criterion(\n",
    "                decoder_output.contiguous().view(-1,\n",
    "                decoder_output.size(-1)), data.contiguous().view(-1)\n",
    "                )\n",
    "            loss /= len(data_lengths)\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    print(\n",
    "        \"Epoch {}, train loss: {:.3f}, val loss: {:.3f}\".format(\n",
    "        i_epoch,\n",
    "        np.mean(train_losses),\n",
    "        np.mean(val_losses))\n",
    "        )\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if i_step == n_steps_max and n_steps_max is not None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  10_43_37_18_20_22_15_38\n",
      "Output: 10_42_37_18_20_6_38_19_38_19_38_19_7_19_7_7_7_7\n",
      "\n",
      "Input:  13_35_23_17_39_47_10_42_1_34_15_20_22_13_15_11_49_34_15_9_42_17_40_21_35_46_42_17_24_15_9_44_14_33_24_15_9_44_25_17_24_15_9_17_36_0_47_24_29_25_17_36_39_2_46_42_40_21_48_37_18_22_5_44_0_37_18_22_15_9_42_47_10_42_37_18_20_22_29_11_45_14_1_34_40_21_0_32_23_37_18_20_6_38\n",
      "Output: 13_35_23_17_36_39_47_47_10_42_47_15_18_20_6_11_14_47_15_9_42_37_18_22_15\n",
      "\n",
      "Input:  16_44_32_43_15_18_6_22_5_11_45_32_43_34_15_9\n",
      "Output: 16_45_14_34_15_9_42_37_18_6_44_0_14_1_10_42_14_1_34_38_38_38_19_38_19\n",
      "\n",
      "Input:  13_41_27_14_47_15_9_46_3_33_10_27_43_15_9_11_45_30_25_40_43\n",
      "Output: 13_41_27_23_47_15_9_3_33_44_0_32_23_47_15_9_42_1_34_15_9_38_38_38_38\n",
      "\n",
      "Input:  13_5_42_47_29_8_44_0_14_10_42_37_22_42_2_23_47_9_38\n",
      "Output: 13_5_47_47_29_44_0_14_1_10_42_47_15_9_42_47_15_38_38_38_38_19_38_19_38\n",
      "\n",
      "Input:  44_0_25_40_10_42_14_47_15_41_37_18_22_15_9_42_37_18_6_22_24_5_21_2_46_42_26_33_10_11_45_14_15_10_40_38\n",
      "Output: 44_0_25_40_21_42_47_15_9_42_37_18_20_6_11_14_47_15_9_42_1_34_15_9_42\n",
      "\n",
      "Input:  28_14_47_15_9_48_45_14_37_18_20_38\n",
      "Output: 28_14_47_15_9_44_0_32_23_18_20_6_38_19_38_19_7_19_7_19_7_7_7_7\n",
      "\n",
      "Input:  13_40_21_2_23_24_15_9_38\n",
      "Output: 13_40_21_2_23_15_9_38_38_38_38_19_38_19_7_19_7_19_7_7_7_10_44_0_47\n",
      "\n",
      "Input:  13_28_1_41_11_45_14_33_36_39_14_1_17_36_39_47_15_20_6_11_2_23_47_24\n",
      "Output: 13_28_1_41_11_45_14_1_10_44_25_17_5_42_47_24_15_9_42_37_18_20_22_15_9\n",
      "\n",
      "Input:  42_1_34_41_11_14_1_10_42_1_10_44_25_40_24_15_9_30_1_10_42_47_18_20_6_22_5_39_2_49_35_49_10_42_47_41_35_46_44_0_30_33_34_18_22_38_13\n",
      "Output: 42_1_34_10_44_14_1_10_44_25_3_17_5_42_47_15_9_42_47_15_9_42_37_18_22\n",
      "\n",
      "Input:  13\n",
      "Output: 13_13_4_4_13_24_5_8_11_2_23_24_5_42_47_24_15_38_38_38_38_38_38_19_38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples without segmentation\n",
    "\n",
    "# Apply to validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "        \n",
    "        y, log_probs = model.decoder.greedy_decode(\n",
    "            encoder_embedding,\n",
    "            max_length=25,\n",
    "            )\n",
    "        x = data.cpu().numpy()\n",
    "        \n",
    "        for i_input in range(y.shape[0]):\n",
    "            # Only print up to EOS symbol\n",
    "            input_symbols = []\n",
    "            for i in x[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                input_symbols.append(id_to_symbol[i])\n",
    "            output_symbols = []\n",
    "            for i in y[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                output_symbols.append(id_to_symbol[i])\n",
    "\n",
    "            print(\"Input: \", \"_\".join(input_symbols))\n",
    "            print(\"Output:\", \"_\".join(output_symbols))\n",
    "            print()\n",
    "            \n",
    "            if i_input == 10:\n",
    "                break\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utterances for evaluation\n",
    "n_eval_utterances = len(prepared_text)  # 1000  # 10000  # 1000\n",
    "# n_eval_utterances = 10000\n",
    "# eval_sentences = prepared_text[-n_eval_utterances:]  # val sentences\n",
    "# eval_utterances = list(utterances)[-n_eval_utterances:]\n",
    "eval_sentences = prepared_text[:n_eval_utterances]\n",
    "eval_utterances = list(utterances)[:n_eval_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 881669/881669 [1:28:47<00:00, 165.49it/s] \n"
     ]
    }
   ],
   "source": [
    "# Embed segments\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Data\n",
    "sentences = eval_sentences\n",
    "# sentences = cur_val_sentences\n",
    "interval_dataset = datasets.SentenceIntervalDataset(\n",
    "    sentences,\n",
    "    text_to_id,\n",
    "    join_char=\"_\"\n",
    "    )\n",
    "segment_loader = DataLoader(\n",
    "    interval_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=datasets.pad_collate,\n",
    "    drop_last=False\n",
    "    )\n",
    "\n",
    "# Apply model to data\n",
    "model.decoder.teacher_forcing_ratio = 1.0\n",
    "model.eval()\n",
    "rnn_losses = []\n",
    "lengths = []\n",
    "eos = []\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(segment_loader)):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        for i_item in range(data.shape[0]):\n",
    "            item_loss = criterion(\n",
    "                decoder_output[i_item].contiguous().view(-1,\n",
    "                decoder_output[i_item].size(-1)),\n",
    "                data[i_item].contiguous().view(-1)\n",
    "                )\n",
    "            rnn_losses.append(item_loss.cpu().numpy())\n",
    "            lengths.append(data_lengths[i_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74785/74785 [01:01<00:00, 1220.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL: -4650737.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Segment\n",
    "\n",
    "# dur_weight = 1.5  # Chorowski\n",
    "dur_weight = 3.0\n",
    "# dur_weight = 1.0\n",
    "\n",
    "i_item = 0\n",
    "losses = []\n",
    "cur_segmented_sentences = []\n",
    "for i_sentence, intervals in enumerate(tqdm(interval_dataset.intervals)):\n",
    "    \n",
    "    # Costs for segment intervals\n",
    "    costs = np.inf*np.ones(len(intervals))\n",
    "    i_eos = intervals[-1][-1]\n",
    "    for i_seg, interval in enumerate(intervals):\n",
    "        if interval is None:\n",
    "            continue\n",
    "        i_start, i_end = interval\n",
    "        dur = i_end - i_start\n",
    "        assert dur == lengths[i_item]\n",
    "        eos = (i_end == i_eos)  # end-of-sequence\n",
    "        \n",
    "        # Chorowski\n",
    "        costs[i_seg] = (\n",
    "            rnn_losses[i_item]\n",
    "            + dur_weight*neg_chorowski(dur)\n",
    "            )\n",
    "        \n",
    "#         # Gamma\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + dur_weight*neg_log_gamma(dur)\n",
    "#             + np.log(np.sum(gamma_cache**dur_weight))\n",
    "#             )\n",
    "        \n",
    "#         # Poisson\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + neg_log_poisson(dur)\n",
    "#             )\n",
    "\n",
    "#         # Histogram\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + dur_weight*(neg_log_hist(dur))\n",
    "#             + np.log(np.sum(histogram**dur_weight))\n",
    "#             )\n",
    "    \n",
    "#         # Sequence boundary\n",
    "#         alpha = 0.3  # 0.3  # 0.9\n",
    "#         if eos:\n",
    "#             costs[i_seg] += -np.log(alpha)\n",
    "#         else:\n",
    "#             costs[i_seg] += -np.log(1 - alpha)\n",
    "# #             K = 5000\n",
    "# #             costs[i_seg] += -np.log((1 - alpha)/K)\n",
    "\n",
    "        # Temp\n",
    "#         if dur > 10 or dur <= 1:\n",
    "#             costs[i_seg] = +np.inf\n",
    "        i_item += 1\n",
    "    \n",
    "    # Viterbi segmentation\n",
    "    n_frames = len(interval_dataset.sentences[i_sentence])\n",
    "    summed_cost, boundaries = viterbi.custom_viterbi(costs, n_frames)\n",
    "    losses.append(summed_cost)\n",
    "    \n",
    "    reference_sentence = sentences[i_sentence]\n",
    "    segmented_sentence = get_segmented_sentence(\n",
    "            interval_dataset.sentences[i_sentence],\n",
    "            boundaries\n",
    "            )\n",
    "    cur_segmented_sentences.append(segmented_sentence)\n",
    "#     # Print examples of the first few sentences\n",
    "#     if i_sentence < 10:\n",
    "#         print(reference_sentence)\n",
    "#         print(segmented_sentence)\n",
    "#         print()\n",
    "    \n",
    "print(\"NLL: {:.4f}\".format(np.sum(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1724_107699-107852:\n",
      "28_26_5_11_2_23_1_34_15 26_5_27_23 17_5_40 18_20_6_33_44_0_25_40_38\n"
     ]
    }
   ],
   "source": [
    "print(f\"{eval_utterances[0]}:\")\n",
    "print(cur_segmented_sentences[0])\n",
    "\n",
    "# # To evaluate gold segmentation:\n",
    "# cur_segmented_sentences = prepared_text_gold[:n_eval_utterances]\n",
    "# print(cur_segmented_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74785it [00:00, 75948.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert segmentation to intervals\n",
    "segmentation_interval_dict = {}\n",
    "for i_utt, utt_key in tqdm(enumerate(eval_utterances)):\n",
    "    words_segmented = cur_segmented_sentences[i_utt].split(\" \")\n",
    "    word_start = 0\n",
    "    word_label = \"\"\n",
    "    i_word = 0\n",
    "    segmentation_interval_dict[utt_key] = []\n",
    "    for (phone_start, phone_end,\n",
    "            phone_label) in phoneseg_interval_dict[utt_key]:\n",
    "        word_label += phone_label + \"_\"\n",
    "        if words_segmented[i_word] == word_label[:-1]:\n",
    "            segmentation_interval_dict[utt_key].append((\n",
    "                word_start, phone_end, word_label[:-1]\n",
    "                ))\n",
    "            word_label = \"\"\n",
    "            word_start = phone_end\n",
    "            i_word += 1\n",
    "\n",
    "#     if i_utt < 10:\n",
    "#         print(segmentation_interval_dict[utt_key])\n",
    "#         print(word_ref_interval_dict[utt_key])\n",
    "#         print()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1117/74785 [00:00<00:06, 11168.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to: ../../vqwordseg/exp/cpc_big/zs2017_en/train/wordseg_segaernn_dp_penalized/intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74785/74785 [00:09<00:00, 7803.06it/s] \n"
     ]
    }
   ],
   "source": [
    "# Write intervals to a directory\n",
    "output_tag = \"wordseg_segaernn_{}\".format(seg_tag.replace(\"phoneseg_\", \"\"))\n",
    "output_dir = (\n",
    "    Path(\"../../vqwordseg/exp\")/vq_model/dataset/split/output_tag/\"intervals\"\n",
    "    )\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Writing to: {output_dir}\")\n",
    "for utt_key in tqdm(segmentation_interval_dict):\n",
    "    with open((output_dir/utt_key).with_suffix(\".txt\"), \"w\") as f:\n",
    "        for (i_segment, (start, end, label)) in enumerate(segmentation_interval_dict[utt_key]):\n",
    "            f.write(f\"{start:d} {end:d} {label}_\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
