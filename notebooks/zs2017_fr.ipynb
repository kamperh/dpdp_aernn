{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation on ZeroSpeech'17 French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Herman Kamper, MIT License\n",
    "\n",
    "Train a segmental autoencoding recurrent neural network (segmental AE-RNN) and perform word segmentation on encoded ZeroSpeech'17 French."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.stats import gamma\n",
    "from sklearn import cluster\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from seg_aernn import datasets, models, viterbi\n",
    "from utils import eval_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_segmented_sentence(ids, boundaries):\n",
    "    output = \"\"\n",
    "    cur_word = []\n",
    "    for i_symbol, boundary in enumerate(boundaries):\n",
    "        cur_word.append(id_to_symbol[ids[i_symbol]])\n",
    "        if boundary:\n",
    "            output += \"_\".join(cur_word)\n",
    "            output += \" \"\n",
    "            cur_word = []\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration penalty functions\n",
    "\n",
    "\"\"\"\n",
    "# Histogram\n",
    "histogram = np.array([\n",
    "    0, 1.66322800e-01, 2.35838129e-01, 2.10609187e-01,\n",
    "    1.48025482e-01, 9.42918160e-02, 5.84211098e-02, 3.64679480e-02,\n",
    "    2.18264741e-02, 1.25420784e-02, 7.18500018e-03, 4.27118399e-03,\n",
    "    1.73743077e-03, 1.19448366e-03, 7.42027726e-04, 2.89571796e-04,\n",
    "    2.35277084e-04, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001\n",
    "    ])  # to-do: check this\n",
    "histogram = histogram/np.sum(histogram)\n",
    "def neg_log_hist(dur):\n",
    "    return -np.log(0 if dur >= len(histogram) else histogram[dur])\n",
    "\n",
    "# Cached Gamma\n",
    "# shape, loc, scale = (2.3, 0, 1.3)  # VQ-VAE\n",
    "shape, loc, scale = (2.6, 0, 1.8)    # CPC-big\n",
    "# shape, loc, scale = (2.5, 0, 1.5)    # CPC-big (Gamma)\n",
    "gamma_cache = []\n",
    "for dur in range(200):\n",
    "    gamma_cache.append(gamma.pdf(dur, shape, loc, scale))\n",
    "gamma_cache = np.array(gamma_cache)/np.sum(gamma_cache)\n",
    "def neg_log_gamma(dur):\n",
    "    if dur < 200:\n",
    "        return -np.log(gamma_cache[dur])\n",
    "    else:\n",
    "        return -np.log(0)\n",
    "\"\"\"\n",
    "    \n",
    "# Chorowski\n",
    "def neg_chorowski(dur):\n",
    "    return -(dur - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "vq_model = \"cpc_big\"\n",
    "# vq_model = \"xlsr\"\n",
    "dataset = \"zs2017_fr\"\n",
    "split = \"train\"\n",
    "seg_tag = \"phoneseg_dp_penalized\"\n",
    "# seg_tag = \"phoneseg_dp_penalized_tune\"\n",
    "\n",
    "# Paths\n",
    "seg_dir = (\n",
    "    Path(\"../../vqwordseg/exp\")/vq_model/dataset/split/seg_tag/\"intervals\"\n",
    "    )\n",
    "# word_ref_dir = Path(\"../../vqwordseg/data\")/dataset/\"word_intervals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47287 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ../../vqwordseg/exp/xlsr/zs2017_fr/train/phoneseg_dp_penalized/intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47287/47287 [00:03<00:00, 13895.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read phone segmentation\n",
    "phoneseg_interval_dict = {}\n",
    "print(\"Reading: {}\".format(seg_dir))\n",
    "phoneseg_interval_dict = eval_segmentation.get_intervals_from_dir(seg_dir)\n",
    "utterances = phoneseg_interval_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47287/47287 [00:00<00:00, 410020.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_26_7_38_23_9_11_49_28_23_36_21_25_30_16_14_16_31_40_6_39_24_35_41_42_4_22_11_20_15_23_38_46_43_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_text = []\n",
    "# n_max_sentence_length = 800\n",
    "for utt_key in tqdm(utterances):\n",
    "    prepared_text.append(\n",
    "        \"_\".join([i[2] for i in phoneseg_interval_dict[utt_key]])\n",
    "#         \"_\".join([i[2] for i in phoneseg_interval_dict[utt_key]][:n_max_sentence_length])\n",
    "        )\n",
    "    \n",
    "print(prepared_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 23, 51, 36, 20, 53, 7, 48, 25, 20, 34, 18, 22, 28, 12, 10, 12, 29, 39, 50, 37, 21, 33, 40, 41, 38, 19, 7, 17, 11, 20, 36, 45, 42, 15]\n",
      "['17', '26', '7', '38', '23', '9', '11', '49', '28', '23', '36', '21', '25', '30', '16', '14', '16', '31', '40', '6', '39', '24', '35', '41', '42', '4', '22', '11', '20', '15', '23', '38', '46', '43', '19']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "PAD_SYMBOL      = \"<pad>\"\n",
    "SOS_SYMBOL      = \"<s>\"    # start of sentence\n",
    "EOS_SYMBOL      = \"</s>\"   # end of sentence\n",
    "BOUNDARY_SYMBOL = \" \"      # word boundary\n",
    "symbols = set()\n",
    "for sentence in prepared_text:\n",
    "    for char in sentence.split(\"_\"):\n",
    "        symbols.add(char)\n",
    "SYMBOLS = [PAD_SYMBOL, SOS_SYMBOL, EOS_SYMBOL, BOUNDARY_SYMBOL] + (sorted(list(symbols)))\n",
    "symbol_to_id = {s: i for i, s in enumerate(SYMBOLS)}\n",
    "id_to_symbol = {i: s for i, s in enumerate(SYMBOLS)}\n",
    "\n",
    "def text_to_id(text, add_sos_eos=False):\n",
    "    \"\"\"\n",
    "    Convert text to a list of symbol IDs.\n",
    "\n",
    "    Sentence start and end symbols can be added by setting `add_sos_eos`.\n",
    "    \"\"\"\n",
    "    symbol_ids = []\n",
    "    for word in text.split(\" \"):\n",
    "        for code in word.split(\"_\"):\n",
    "            symbol_ids.append(symbol_to_id[code])\n",
    "        symbol_ids.append(symbol_to_id[BOUNDARY_SYMBOL])\n",
    "    symbol_ids = symbol_ids[:-1]  # remove last space\n",
    "\n",
    "    if add_sos_eos:\n",
    "        return [symbol_to_id[SOS_SYMBOL]] + symbol_ids + [symbol_to_id[EOS_SYMBOL]]\n",
    "    else:\n",
    "        return symbol_ids\n",
    "\n",
    "print(text_to_id(prepared_text[0]))\n",
    "print([id_to_symbol[i] for i in text_to_id(prepared_text[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_26_7_38_23_9_11_49_28_23_36_21_25_30_16_14_16_31_40_6_39_24_35_41_42_4_22_11_20_15_23_38_46_43_19\n",
      "17_26_7_18_22_29_13_48_8_33_35_47_35_40_22_45_1_15_43_32_28_38_4_11_49_1_23_9_23_12_25_13_39_24_49_2_32_28_36_47_21_45_2_31_38_11_18_31_9_11_18_30_16_34_42_12_2_31_16_47_16_38_9_11_49_5_2_32_28_38_12_46_14_46_43_14_19\n",
      "17_26_7_42_37_48_44_9_4_0_24_49_43_28_10_13_24_22_44_33_18_16_14_34_33_18_30_16_47_34_42_0_18_16_47_16\n",
      "17_14_26_7_1_6_46\n",
      "17_26_7_15_27_13_18_22_35_36_22_0_8_41_42_0_18_30_16_34_38_9_11_18_16_14_10_34_9_11_44_27_39_37_48_44_9_11_49_20_43_28_41_42_0_37_44_13_33_24_15_27_36_25_16_22_16_47_10_13_18_16_47_31_40_24_44_45_48_45_22_45_46_14_19\n",
      "17_26_25_36_24_20_28_31_9_37_15_27_13_18_14_16_34_40_49_43_14_31_40_11_49_28_41_42_0_37_22_44_1_45_33_9_46\n",
      "17_26_35_33_37_48_44_41_9_48_11_15_20_27_13_24_44_41_38_25_30_23_40_38_36_6_18_16_30_47_16_3_43_14_3_19\n"
     ]
    }
   ],
   "source": [
    "# First three words of training data\n",
    "word_dataset = datasets.WordDataset(prepared_text, text_to_id)\n",
    "for i in range(7):\n",
    "    sample = word_dataset[i]\n",
    "    print(\"_\".join([id_to_symbol[i] for i in sample.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. train sentences: 47287\n",
      "Examples: ['17_26_7_38_23_9_11_49_28_23_36_21_25_30_16_14_16_31_40_6_39_24_35_41_42_4_22_11_20_15_23_38_46_43_19', '17_26_7_18_22_29_13_48_8_33_35_47_35_40_22_45_1_15_43_32_28_38_4_11_49_1_23_9_23_12_25_13_39_24_49_2_32_28_36_47_21_45_2_31_38_11_18_31_9_11_18_30_16_34_42_12_2_31_16_47_16_38_9_11_49_5_2_32_28_38_12_46_14_46_43_14_19', '17_26_7_42_37_48_44_9_4_0_24_49_43_28_10_13_24_22_44_33_18_16_14_34_33_18_30_16_47_34_42_0_18_16_47_16']\n",
      "Min length:  1\n",
      "Max length:  239\n",
      "Mean length: 39.6244\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "cur_val_sentences = prepared_text[-1000:]\n",
    "cur_train_sentences = prepared_text\n",
    "# cur_train_sentences = [i for i in prepared_text if len(i.split(\"_\")) <= 50]  # temp\n",
    "\n",
    "# Random boundaries\n",
    "np.random.seed(42)\n",
    "# cur_train_sentences = insert_random_boundaries(cur_train_sentences)\n",
    "# cur_val_sentences = insert_random_boundaries(cur_val_sentences)\n",
    "\n",
    "print(\"No. train sentences:\", len(cur_train_sentences))\n",
    "print(\"Examples:\", cur_train_sentences[:3])\n",
    "print(\"Min length: \", min([len(i.split(\"_\")) for i in cur_train_sentences]))\n",
    "print(\"Max length: \", max([len(i.split(\"_\")) for i in cur_train_sentences]))\n",
    "print(\"Mean length: {:.4f}\".format(np.mean([len(i.split(\"_\")) for i in cur_train_sentences])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(i.split(\"_\")) for i in cur_train_sentences], 200, density=True)\n",
    "# plt.xlim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE-RNN model\n",
    "n_symbols = len(SYMBOLS)\n",
    "symbol_embedding_dim = 10  # 25\n",
    "hidden_dim = 500  # 250  # 500  # 1000  # 200\n",
    "embedding_dim = 50  # 150  # 300  # 25\n",
    "teacher_forcing_ratio = 0.5  # 1.0  # 0.5  # 1.0\n",
    "n_encoder_layers = 1  # 1  # 3  # 10\n",
    "n_decoder_layers = 1  # 1  # 1\n",
    "batch_size = 32  # 32*3  # 32\n",
    "learning_rate = 0.001\n",
    "input_dropout = 0.0  # 0.0 # 0.5\n",
    "dropout = 0.0\n",
    "n_symbols_max = 50  # 25\n",
    "# n_epochs_max = 5\n",
    "n_epochs_max = None  # determined from n_max_steps and batch size\n",
    "n_steps_max = 1500  # 2500  # 1500  # 1000  # None\n",
    "# n_steps_max = None  # Only use n_epochs_max\n",
    "bidirectional_encoder = False  # False\n",
    "\n",
    "encoder = models.Encoder(\n",
    "    n_symbols=n_symbols,\n",
    "    symbol_embedding_dim=symbol_embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers=n_encoder_layers,\n",
    "    dropout=dropout,\n",
    "    input_dropout=input_dropout,\n",
    "    bidirectional=bidirectional_encoder\n",
    "    )\n",
    "# decoder = models.Decoder1(\n",
    "#     n_symbols=n_symbols,\n",
    "#     symbol_embedding_dim=symbol_embedding_dim,\n",
    "#     hidden_dim=hidden_dim,\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     n_layers=n_decoder_layers,\n",
    "#     sos_id = symbol_to_id[SOS_SYMBOL],\n",
    "#     teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "#     dropout=dropout\n",
    "#     )\n",
    "decoder = models.Decoder2(\n",
    "    n_symbols=n_symbols,\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers=n_decoder_layers,\n",
    "    dropout=dropout\n",
    "    )\n",
    "model = models.EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1478/1478 [00:45<00:00, 32.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 110.240, val loss: 122.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 21/1478 [00:00<00:46, 31.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 102.575, val loss: 124.880\n"
     ]
    }
   ],
   "source": [
    "# Training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Training data\n",
    "train_dataset = datasets.WordDataset(\n",
    "    cur_train_sentences, text_to_id, n_symbols_max=n_symbols_max\n",
    "    )\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=datasets.pad_collate\n",
    "    )\n",
    "\n",
    "# Validation data\n",
    "val_dataset = datasets.WordDataset(cur_val_sentences, text_to_id)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=datasets.pad_collate\n",
    "    )\n",
    "\n",
    "# Loss\n",
    "criterion = nn.NLLLoss(\n",
    "    reduction=\"sum\", ignore_index=symbol_to_id[PAD_SYMBOL]\n",
    "    )\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "if n_epochs_max is None:\n",
    "    steps_per_epoch = np.ceil(len(cur_train_sentences)/batch_size)\n",
    "    n_epochs_max = int(np.ceil(n_steps_max/steps_per_epoch))\n",
    "\n",
    "i_step = 0\n",
    "for i_epoch in range(n_epochs_max):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(train_loader)):\n",
    "        optimiser.zero_grad()\n",
    "        data = data.to(device)       \n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_output.contiguous().view(-1, decoder_output.size(-1)),\n",
    "            data.contiguous().view(-1)\n",
    "            )\n",
    "        loss /= len(data_lengths)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_losses.append(loss.item())\n",
    "        i_step += 1\n",
    "        if i_step == n_steps_max and n_steps_max is not None:\n",
    "            break\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (data, data_lengths) in enumerate(val_loader):\n",
    "            data = data.to(device)            \n",
    "            encoder_embedding, decoder_output = model(\n",
    "                data, data_lengths, data, data_lengths\n",
    "                )\n",
    "\n",
    "            loss = criterion(\n",
    "                decoder_output.contiguous().view(-1,\n",
    "                decoder_output.size(-1)), data.contiguous().view(-1)\n",
    "                )\n",
    "            loss /= len(data_lengths)\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    print(\n",
    "        \"Epoch {}, train loss: {:.3f}, val loss: {:.3f}\".format(\n",
    "        i_epoch,\n",
    "        np.mean(train_losses),\n",
    "        np.mean(val_losses))\n",
    "        )\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if i_step == n_steps_max and n_steps_max is not None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  17_26_40_8_13_6_37_1_27_33_8_42_0_49_22_2_14_28_31_8_45_22_45_46_47_19\n",
      "Output: 17_26_7_37_37_35_35_35_13_13_39_39_49_49_2_2_28_28_10_10_45_45_45_46_46\n",
      "\n",
      "Input:  26_9_11_2_31_38_22_23_40_18_16_14_31_9_11_48_27_41_42_0_37_41_42_0_49_22_2_41_42_25_5_25_21_1_45_47_45_46_19\n",
      "Output: 26_42_18_28_28_38_38_11_11_11_38_38_38_38_38_38_38_38_38_38_38_38_38_38_38\n",
      "\n",
      "Input:  17_26_7_36_42_25_20_2_14_31_40_33_37_27_39_18_16_1_34_39_37_22_44_12_22_12_18_5_3_14_3_19\n",
      "Output: 17_26_7_7_33_49_20_2_28_31_36_0_37_44_44_44_33_33_33_33_33_33_16_22_22\n",
      "\n",
      "Input:  17_26_38_37_1_44_33_37_13_24_20_14_28_34_36_21_46_19\n",
      "Output: 17_26_42_37_27_27_27_39_49_20_14_14_28_10_13_45_46_46_19_19_19_19_19_19_19\n",
      "\n",
      "Input:  17_26_31_40_6_27_39_24_15_27_39_6_2_28_36_0_49_20_43_28_34_6_5_35_15_27_33_49_20_2_28_40_23_33_44_48_13_1_33_23_5_23_33_15_22_27_39_24_18_5_16_14_16_10_6_49_2_14_28_43_28_10_13_2_28_6_15_27_39_24_49_28_16_2_31_36_24_18_1_16_10_33_9_11_29_1_29_34_42_4_1_25_11_48_44_9_11_2_43_3_19\n",
      "Output: 17_26_7_8_37_35_33_33_49_49_2_28_28_31_38_6_2_2_2_2_2_2_2_2_2\n",
      "\n",
      "Input:  17_26_29_36_6_48_44_42_0_49_20_2_28_34_39_8_49_43_14_32_28_42_25_20_16_14_31_42_37_44_9_22_12_15_2_32_28_31_30_31_40_46_22_46_22_46_43_19\n",
      "Output: 17_26_7_8_44_44_0_0_49_2_2_28_31_42_0_49_2_28_28_28_28_28_28_28_28\n",
      "\n",
      "Input:  17_31_38_11_37_27_33_49_43_28_41_0_30_35_39_24_18_14_31_39_24_22_13_39_24_44_39_24_49_2_28_31_9_11_49_43_28_41_42_21_37_47_45_47_45_21_47_19\n",
      "Output: 17_26_38_38_0_49_20_2_28_31_38_24_24_39_39_39_39_39_39_39_39_39_39_39_24\n",
      "\n",
      "Input:  17_26_42_0_44_41_36_49_20_2_28_31_14_23_9_11_5_16_47_34_36_24_15_27_33_8_41_42_0_37_22_15_40_5_40_22_40_23_21_24_15_27_39_18_16_14_16_43_28_34_8_42_1_4_35_30_35_46_19\n",
      "Output: 17_26_42_37_44_0_0_49_2_28_28_34_38_37_8_44_41_41_41_41_41_41_41_41_41\n",
      "\n",
      "Input:  17_26_44_41_36_0_18_14_16_34_44_22_41_33_5_35_30_35_12_48_12_11_23_46_19\n",
      "Output: 17_26_42_38_11_18_18_16_14_34_38_38_37_37_41_38_38_38_41_41_41_41_12_12_46\n",
      "\n",
      "Input:  17_26_40_23_22_31_14_31_41_36_25_2_31_8_42_0_18_5_16_34_0_8_42_30_25_21_18_22_16_3_14_3_22_46_19\n",
      "Output: 17_26_42_0_49_2_28_31_42_0_0_41_41_42_42_42_42_42_42_42_42_42_42_16_16\n",
      "\n",
      "Input:  17_26_14_34_7_24_20_28_41_42_0_18_16_22_34_45_30_45_46_43_19\n",
      "Output: 17_26_7_7_7_20_2_28_31_36_0_0_18_16_14_14_14_45_46_46_19_19_19_19_19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples without segmentation\n",
    "\n",
    "# Apply to validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "        \n",
    "        y, log_probs = model.decoder.greedy_decode(\n",
    "            encoder_embedding,\n",
    "            max_length=25,\n",
    "            )\n",
    "        x = data.cpu().numpy()\n",
    "        \n",
    "        for i_input in range(y.shape[0]):\n",
    "            # Only print up to EOS symbol\n",
    "            input_symbols = []\n",
    "            for i in x[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                input_symbols.append(id_to_symbol[i])\n",
    "            output_symbols = []\n",
    "            for i in y[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                output_symbols.append(id_to_symbol[i])\n",
    "\n",
    "            print(\"Input: \", \"_\".join(input_symbols))\n",
    "            print(\"Output:\", \"_\".join(output_symbols))\n",
    "            print()\n",
    "            \n",
    "            if i_input == 10:\n",
    "                break\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utterances for evaluation\n",
    "n_eval_utterances = len(prepared_text)  # 1000  # 10000  # 1000\n",
    "# eval_sentences = prepared_text[-n_eval_utterances:]  # val sentences\n",
    "# eval_utterances = list(utterances)[-n_eval_utterances:]\n",
    "eval_sentences = prepared_text[:n_eval_utterances]\n",
    "eval_utterances = list(utterances)[:n_eval_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727516/727516 [1:02:40<00:00, 193.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Embed segments\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Data\n",
    "sentences = eval_sentences\n",
    "# sentences = cur_val_sentences\n",
    "interval_dataset = datasets.SentenceIntervalDataset(\n",
    "    sentences,\n",
    "    text_to_id,\n",
    "    join_char=\"_\"\n",
    "    )\n",
    "segment_loader = DataLoader(\n",
    "    interval_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=datasets.pad_collate,\n",
    "    drop_last=False\n",
    "    )\n",
    "\n",
    "# Apply model to data\n",
    "model.decoder.teacher_forcing_ratio = 1.0\n",
    "model.eval()\n",
    "rnn_losses = []\n",
    "lengths = []\n",
    "eos = []\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(segment_loader)):\n",
    "        data = data.to(device)\n",
    "\n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        for i_item in range(data.shape[0]):\n",
    "            item_loss = criterion(\n",
    "                decoder_output[i_item].contiguous().view(-1,\n",
    "                decoder_output[i_item].size(-1)),\n",
    "                data[i_item].contiguous().view(-1)\n",
    "                )\n",
    "            rnn_losses.append(item_loss.cpu().numpy())\n",
    "            lengths.append(data_lengths[i_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47287/47287 [00:45<00:00, 1034.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL: -435883.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Segment\n",
    "\n",
    "dur_weight = 3.0  # 3.0\n",
    "\n",
    "i_item = 0\n",
    "losses = []\n",
    "cur_segmented_sentences = []\n",
    "for i_sentence, intervals in enumerate(tqdm(interval_dataset.intervals)):\n",
    "    \n",
    "    # Costs for segment intervals\n",
    "    costs = np.inf*np.ones(len(intervals))\n",
    "    i_eos = intervals[-1][-1]\n",
    "    for i_seg, interval in enumerate(intervals):\n",
    "        if interval is None:\n",
    "            continue\n",
    "        i_start, i_end = interval\n",
    "        dur = i_end - i_start\n",
    "        assert dur == lengths[i_item]\n",
    "        eos = (i_end == i_eos)  # end-of-sequence\n",
    "        \n",
    "        # Chorowski\n",
    "        costs[i_seg] = (\n",
    "            rnn_losses[i_item]\n",
    "            + dur_weight*neg_chorowski(dur)\n",
    "            )\n",
    "        \n",
    "#         # Gamma\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + dur_weight*neg_log_gamma(dur)\n",
    "#             + np.log(np.sum(gamma_cache**dur_weight))\n",
    "#             )\n",
    "        \n",
    "#         # Poisson\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + neg_log_poisson(dur)\n",
    "#             )\n",
    "\n",
    "#         # Histogram\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + dur_weight*(neg_log_hist(dur))\n",
    "#             + np.log(np.sum(histogram**dur_weight))\n",
    "#             )\n",
    "    \n",
    "#         # Sequence boundary\n",
    "#         alpha = 0.3  # 0.3  # 0.9\n",
    "#         if eos:\n",
    "#             costs[i_seg] += -np.log(alpha)\n",
    "#         else:\n",
    "#             costs[i_seg] += -np.log(1 - alpha)\n",
    "# #             K = 5000\n",
    "# #             costs[i_seg] += -np.log((1 - alpha)/K)\n",
    "\n",
    "        # Temp\n",
    "#         if dur > 10 or dur <= 1:\n",
    "#             costs[i_seg] = +np.inf\n",
    "        i_item += 1\n",
    "    \n",
    "    # Viterbi segmentation\n",
    "    n_frames = len(interval_dataset.sentences[i_sentence])\n",
    "    summed_cost, boundaries = viterbi.custom_viterbi(costs, n_frames)\n",
    "    losses.append(summed_cost)\n",
    "    \n",
    "    reference_sentence = sentences[i_sentence]\n",
    "    segmented_sentence = get_segmented_sentence(\n",
    "            interval_dataset.sentences[i_sentence],\n",
    "            boundaries\n",
    "            )\n",
    "    cur_segmented_sentences.append(segmented_sentence)\n",
    "#     # Print examples of the first few sentences\n",
    "#     if i_sentence < 10:\n",
    "#         print(reference_sentence)\n",
    "#         print(segmented_sentence)\n",
    "#         print()\n",
    "\n",
    "print(\"NLL: {:.4f}\".format(np.sum(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M05_R_540774-540911:\n",
      "17_26_7_38_23_9_11_49_28_23 36_21_25 30_16_14_16_31_40_6 39_24_35 41_42_4_22_11 20_15_23_38_46_43_19\n"
     ]
    }
   ],
   "source": [
    "print(f\"{eval_utterances[0]}:\")\n",
    "print(cur_segmented_sentences[0])\n",
    "\n",
    "# # To evaluate gold segmentation:\n",
    "# cur_segmented_sentences = prepared_text_gold[:n_eval_utterances]\n",
    "# print(cur_segmented_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47287it [00:00, 67439.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert segmentation to intervals\n",
    "segmentation_interval_dict = {}\n",
    "for i_utt, utt_key in tqdm(enumerate(eval_utterances)):\n",
    "    words_segmented = cur_segmented_sentences[i_utt].split(\" \")\n",
    "    word_start = 0\n",
    "    word_label = \"\"\n",
    "    i_word = 0\n",
    "    segmentation_interval_dict[utt_key] = []\n",
    "    for (phone_start, phone_end,\n",
    "            phone_label) in phoneseg_interval_dict[utt_key]:\n",
    "        word_label += phone_label + \"_\"\n",
    "        if words_segmented[i_word] == word_label[:-1]:\n",
    "            segmentation_interval_dict[utt_key].append((\n",
    "                word_start, phone_end, word_label[:-1]\n",
    "                ))\n",
    "            word_label = \"\"\n",
    "            word_start = phone_end\n",
    "            i_word += 1\n",
    "\n",
    "#     if i_utt < 10:\n",
    "#         print(segmentation_interval_dict[utt_key])\n",
    "#         print(word_ref_interval_dict[utt_key])\n",
    "#         print()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1133/47287 [00:00<00:04, 11329.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to: ../../vqwordseg/exp/xlsr/zs2017_fr/train/wordseg_segaernn_dp_penalized/intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47287/47287 [00:05<00:00, 9348.41it/s] \n"
     ]
    }
   ],
   "source": [
    "# Write intervals to a directory\n",
    "output_tag = \"wordseg_segaernn_{}\".format(seg_tag.replace(\"phoneseg_\", \"\"))\n",
    "output_dir = (\n",
    "    Path(\"../../vqwordseg/exp\")/vq_model/dataset/split/output_tag/\"intervals\"\n",
    "    )\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Writing to: {output_dir}\")\n",
    "for utt_key in tqdm(segmentation_interval_dict):\n",
    "    with open((output_dir/utt_key).with_suffix(\".txt\"), \"w\") as f:\n",
    "        for (i_segment, (start, end, label)) in enumerate(segmentation_interval_dict[utt_key]):\n",
    "            f.write(f\"{start:d} {end:d} {label}_\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
