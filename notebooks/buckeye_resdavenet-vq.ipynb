{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation on Buckeye ResDAVEnet-VQ Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herman Kamper, 2021\n",
    "\n",
    "Train a segmental autoencoding recurrent neural network (segmental AE-RNN) and perform word segmentation on encoded Buckeye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.stats import gamma\n",
    "from sklearn import cluster\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from seg_aernn import datasets, models, viterbi\n",
    "from utils import eval_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmented_sentence(ids, boundaries):\n",
    "    output = \"\"\n",
    "    cur_word = []\n",
    "    for i_symbol, boundary in enumerate(boundaries):\n",
    "        cur_word.append(id_to_symbol[ids[i_symbol]])\n",
    "        if boundary:\n",
    "            output += \"_\".join(cur_word)\n",
    "            output += \" \"\n",
    "            cur_word = []\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration penalty functions\n",
    "\n",
    "# Histogram\n",
    "histogram = np.array([\n",
    "    0, 1.66322800e-01, 2.35838129e-01, 2.10609187e-01,\n",
    "    1.48025482e-01, 9.42918160e-02, 5.84211098e-02, 3.64679480e-02,\n",
    "    2.18264741e-02, 1.25420784e-02, 7.18500018e-03, 4.27118399e-03,\n",
    "    1.73743077e-03, 1.19448366e-03, 7.42027726e-04, 2.89571796e-04,\n",
    "    2.35277084e-04, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001\n",
    "    ])  # to-do: check this\n",
    "histogram = histogram/np.sum(histogram)\n",
    "def neg_log_hist(dur):\n",
    "    return -np.log(0 if dur >= len(histogram) else histogram[dur])\n",
    "\n",
    "# Cached Gamma\n",
    "# shape, loc, scale = (2.3, 0, 1.3)  # VQ-VAE\n",
    "# shape, loc, scale = (2.6, 0, 1.8)    # CPC-big\n",
    "shape, loc, scale = (2.4, 0, 2.4)    # ResDAVEnet-VQ\n",
    "# shape, loc, scale = (2.6, 0, 2.4)    # ResDAVEnet-VQ clustered\n",
    "# shape, loc, scale = (2.1, 0, 1.3)    # ResDAVEnet-VQ quant 3\n",
    "gamma_cache = []\n",
    "for dur in range(200):\n",
    "    gamma_cache.append(gamma.pdf(dur, shape, loc, scale))\n",
    "gamma_cache = np.array(gamma_cache)/np.sum(gamma_cache)\n",
    "def neg_log_gamma(dur):\n",
    "    if dur < 200:\n",
    "        return -np.log(gamma_cache[dur])\n",
    "    else:\n",
    "        return -np.log(0)\n",
    "    \n",
    "# Chorowski\n",
    "def neg_chorowski(dur):\n",
    "    return -(dur - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "vq_model = \"resdavenet_vq\"\n",
    "# vq_model = \"resdavenet_vq_clust50\"\n",
    "# vq_model = \"resdavenet_vq_quant3\"\n",
    "dataset = \"buckeye\"\n",
    "split = \"val\"\n",
    "# seg_tag = \"phoneseg_dp_penalized\"\n",
    "seg_tag = \"phoneseg_merge\"\n",
    "# seg_tag = \"phoneseg_dp_penalized_tune\"\n",
    "\n",
    "# Paths\n",
    "seg_dir = (\n",
    "    Path(\"../../vqwordseg/exp\")/vq_model/dataset/split/seg_tag/\"intervals\"\n",
    "    )\n",
    "word_ref_dir = Path(\"../../vqwordseg/data\")/dataset/\"word_intervals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/16582 [00:00<03:03, 90.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ../../vqwordseg/exp/resdavenet_vq/buckeye/val/phoneseg_merge/intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16582/16582 [02:29<00:00, 110.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read phone segmentation\n",
    "phoneseg_interval_dict = {}\n",
    "print(\"Reading: {}\".format(seg_dir))\n",
    "phoneseg_interval_dict = eval_segmentation.get_intervals_from_dir(seg_dir)\n",
    "utterances = phoneseg_interval_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16582 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: ../../vqwordseg/data/buckeye/word_intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16582/16582 [00:00<00:00, 39258.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read word reference\n",
    "print(\"Reading: {}\".format(word_ref_dir))\n",
    "word_ref_interval_dict = eval_segmentation.get_intervals_from_dir(word_ref_dir, utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16582/16582 [00:00<00:00, 1048544.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. word types: 4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_types = set()\n",
    "for utt_key in tqdm(word_ref_interval_dict):\n",
    "    for start, end, label in word_ref_interval_dict[utt_key]:\n",
    "        word_types.add(label)\n",
    "print(\"No. word types:\", len(word_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16582/16582 [00:00<00:00, 305488.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert intervals to boundaries\n",
    "word_ref_boundaries_dict = {}\n",
    "for utt_key in tqdm(word_ref_interval_dict):\n",
    "    word_ref_boundaries_dict[utt_key] = eval_segmentation.intervals_to_boundaries(\n",
    "        word_ref_interval_dict[utt_key]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16582/16582 [00:00<00:00, 471960.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598_930_86_812_68_263_512_658_652_772_484_502_474_672_120_332_736_234_1014_740_340_54_126_190_115_176_908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prepared_text = []\n",
    "for utt_key in tqdm(utterances):\n",
    "    prepared_text.append(\n",
    "        \"_\".join([i[2] for i in phoneseg_interval_dict[utt_key]])\n",
    "        )\n",
    "    \n",
    "print(prepared_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16582/16582 [00:00<00:00, 21237.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598_930_86_812_68_263_512_658 652_772_484_502_474_672_120_332_736_234_1014_740_340_54 126_190_115_176_908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gold segmentation, where boundaries are inserted in best possible positions\n",
    "n_not_in_tolerance = 0\n",
    "prepared_text_gold = []\n",
    "for utt_key in tqdm(utterances):\n",
    "    seg_intervals = phoneseg_interval_dict[utt_key].copy()\n",
    "    ref_intervals = word_ref_interval_dict[utt_key].copy()\n",
    "    seg_boundaries = np.array([i[1] - 1 for i in seg_intervals])\n",
    "    ref_boundaries = np.array([i[1] - 1 for i in ref_intervals])\n",
    "    for ref_boundary in ref_boundaries[:-1]:\n",
    "        i_seg = np.argmin(np.abs(seg_boundaries - ref_boundary))\n",
    "        seg_intervals.insert(\n",
    "            i_seg + 1, (seg_intervals[i_seg][1], seg_intervals[i_seg][1], \" \")\n",
    "            )\n",
    "        seg_boundaries = np.array([i[1] - 1 for i in seg_intervals])\n",
    "    cur_text_gold = \"\"\n",
    "    for start, end, label in seg_intervals:\n",
    "        if label == \" \":\n",
    "            cur_text_gold = cur_text_gold[:-1]\n",
    "            cur_text_gold += \" \"\n",
    "        else:\n",
    "            cur_text_gold += label + \"_\"\n",
    "    cur_text_gold = cur_text_gold[:-1]\n",
    "    prepared_text_gold.append(cur_text_gold)\n",
    "\n",
    "print(prepared_text_gold[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. word types: 55201\n",
      "Mean training word length: 9.2996\n",
      "Min training word length:  1\n",
      "Max training word length:  53\n",
      "Mean: 9.299609790705924\n",
      "Gamma parameters: 2.5457309448269023 0 3.6530214670184846\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training word length statistics\n",
    "word_lengths = []\n",
    "n_words = []\n",
    "word_types = set()\n",
    "for sentence in prepared_text_gold:\n",
    "    word_lengths.extend([len(i.split(\"_\")) for i in sentence.split(\" \")])\n",
    "    n_words.append(len(sentence.split(\" \")))\n",
    "    for word in sentence.split(\" \"):\n",
    "        word_types.add(word)\n",
    "#     word_lengths.extend([len(i.replace(\"$\", \"\")) for i in sentence.split(\" \")])  # temp\n",
    "print(\"No. word types:\", len(word_types))\n",
    "print(\"Mean training word length: {:.4f}\".format(np.mean(word_lengths)))\n",
    "print(\"Min training word length:  {:d}\".format(np.min(word_lengths)))\n",
    "print(\"Max training word length:  {:d}\".format(np.max(word_lengths)))\n",
    "\n",
    "# Histogram\n",
    "plt.figure()\n",
    "n, bins, patches = plt.hist(word_lengths, bins=range(20), density=True)\n",
    "plt.title(\"Word lengths\")\n",
    "\n",
    "# Gamma\n",
    "mean = np.mean(word_lengths)\n",
    "var  = np.var(word_lengths)\n",
    "alpha = (mean**2)/var\n",
    "beta  = alpha / mean\n",
    "shape = alpha\n",
    "loc = 0\n",
    "scale = 1/beta\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Gamma parameters:\", shape, loc, scale)\n",
    "# shape, loc, scale = (2.4, 0, 1.9)\n",
    "plt.plot(bins, gamma.pdf(bins, shape, loc, scale))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[271, 442, 405, 381, 314, 108, 228, 303, 301, 360, 217, 224, 212, 311, 28, 142, 342, 90, 13, 345, 146, 243, 31, 66, 24, 58, 431]\n",
      "['598', '930', '86', '812', '68', '263', '512', '658', '652', '772', '484', '502', '474', '672', '120', '332', '736', '234', '1014', '740', '340', '54', '126', '190', '115', '176', '908']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "PAD_SYMBOL      = \"<pad>\"\n",
    "SOS_SYMBOL      = \"<s>\"    # start of sentence\n",
    "EOS_SYMBOL      = \"</s>\"   # end of sentence\n",
    "BOUNDARY_SYMBOL = \" \"      # word boundary\n",
    "symbols = set()\n",
    "for sentence in prepared_text:\n",
    "    for char in sentence.split(\"_\"):\n",
    "        symbols.add(char)\n",
    "SYMBOLS = [PAD_SYMBOL, SOS_SYMBOL, EOS_SYMBOL, BOUNDARY_SYMBOL] + (sorted(list(symbols)))\n",
    "symbol_to_id = {s: i for i, s in enumerate(SYMBOLS)}\n",
    "id_to_symbol = {i: s for i, s in enumerate(SYMBOLS)}\n",
    "\n",
    "def text_to_id(text, add_sos_eos=False):\n",
    "    \"\"\"\n",
    "    Convert text to a list of symbol IDs.\n",
    "\n",
    "    Sentence start and end symbols can be added by setting `add_sos_eos`.\n",
    "    \"\"\"\n",
    "    symbol_ids = []\n",
    "    for word in text.split(\" \"):\n",
    "        for code in word.split(\"_\"):\n",
    "            symbol_ids.append(symbol_to_id[code])\n",
    "        symbol_ids.append(symbol_to_id[BOUNDARY_SYMBOL])\n",
    "    symbol_ids = symbol_ids[:-1]  # remove last space\n",
    "\n",
    "    if add_sos_eos:\n",
    "        return [symbol_to_id[SOS_SYMBOL]] + symbol_ids + [symbol_to_id[EOS_SYMBOL]]\n",
    "    else:\n",
    "        return symbol_ids\n",
    "\n",
    "print(text_to_id(prepared_text[0]))\n",
    "print([id_to_symbol[i] for i in text_to_id(prepared_text[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598_930_86_812_68_263_512_658_652_772_484_502_474_672_120_332_736_234_1014_740_340_54_126_190_115_176_908\n",
      "228_648_498_492_316_534_714_680_748_716_960_509_100\n",
      "854_116_752_1010_932_232_1021_154_716_924_28_776_758_156_268_136_430_855_995_960_848_364_524_874_546_74_988_298_474_672_367_552_776_252_920_660_244_285_416\n",
      "46_34_712_180_448_988_314_100_866_68_508_804_930_856_726_518_980_378_990_210\n",
      "726_812_184_16_726_676_560_706_512_658_668_594_40_580_732_706_500_392_44_970_142_454_904_500_392_248_652_482_1002_388_262_58_928_71_458_995_96_644_636_28_776_360_313_168_64_804\n",
      "982_30_180_364_666_518_608_876_68_628_380_772_1012_70_614_300_971_706_66_506_362_698_522\n",
      "272_968_54_160_940_498_188_786_810_58_396_484_1012_260_300_448_874_792_172_432_532_528_60_220_568_546_36_806_736_848_16_564_1012_288_204_802_164_788_240_765_648_498_644_316_786_362_122_76_482_772_482_962_222_118\n"
     ]
    }
   ],
   "source": [
    "# First three words of training data\n",
    "word_dataset = datasets.WordDataset(prepared_text, text_to_id)\n",
    "for i in range(7):\n",
    "    sample = word_dataset[i]\n",
    "    print(\"_\".join([id_to_symbol[i] for i in sample.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. train sentences: 10000\n",
      "Examples: ['598_930_86_812_68_263_512_658_652_772_484_502_474_672_120_332_736_234_1014_740_340_54_126_190_115_176_908', '228_648_498_492_316_534_714_680_748_716_960_509_100', '854_116_752_1010_932_232_1021_154_716_924_28_776_758_156_268_136_430_855_995_960_848_364_524_874_546_74_988_298_474_672_367_552_776_252_920_660_244_285_416']\n",
      "Min length:  1\n",
      "Max length:  294\n",
      "Mean length: 31.5690\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "# Approximate ground truth (for debugging)\n",
    "# cur_train_sentences = prepared_text_gold[:10000]\n",
    "cur_val_sentences = prepared_text_gold[-1000:]\n",
    "\n",
    "# No boundaries\n",
    "cur_train_sentences = prepared_text[:10000]\n",
    "# cur_val_sentences = prepared_text[-1000:]\n",
    "\n",
    "# Random boundaries\n",
    "np.random.seed(42)\n",
    "# cur_train_sentences = insert_random_boundaries(cur_train_sentences)\n",
    "# cur_val_sentences = insert_random_boundaries(cur_val_sentences)\n",
    "\n",
    "print(\"No. train sentences:\", len(cur_train_sentences))\n",
    "print(\"Examples:\", cur_train_sentences[:3])\n",
    "print(\"Min length: \", min([len(i.split(\"_\")) for i in cur_train_sentences]))\n",
    "print(\"Max length: \", max([len(i.split(\"_\")) for i in cur_train_sentences]))\n",
    "print(\"Mean length: {:.4f}\".format(np.mean([len(i.split(\"_\")) for i in cur_train_sentences])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE-RNN model\n",
    "n_symbols = len(SYMBOLS)\n",
    "symbol_embedding_dim = 10  # 25\n",
    "hidden_dim = 500  # 250  # 500  # 1000  # 200\n",
    "embedding_dim = 50  # 150  # 300  # 25\n",
    "teacher_forcing_ratio = 0.5  # 1.0  # 0.5  # 1.0\n",
    "n_encoder_layers = 1  # 1  # 3  # 10\n",
    "n_decoder_layers = 1  # 1  # 1\n",
    "batch_size = 32  # 32*3  # 32\n",
    "learning_rate = 0.001\n",
    "input_dropout = 0.0  # 0.0 # 0.5\n",
    "dropout = 0.0\n",
    "n_symbols_max = 25\n",
    "n_epochs_max = 5\n",
    "bidirectional_encoder = False  # False\n",
    "\n",
    "encoder = models.Encoder(\n",
    "    n_symbols=n_symbols,\n",
    "    symbol_embedding_dim=symbol_embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers=n_encoder_layers,\n",
    "    dropout=dropout,\n",
    "    input_dropout=input_dropout,\n",
    "    bidirectional=bidirectional_encoder\n",
    "    )\n",
    "# decoder = models.Decoder1(\n",
    "#     n_symbols=n_symbols,\n",
    "#     symbol_embedding_dim=symbol_embedding_dim,\n",
    "#     hidden_dim=hidden_dim,\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     n_layers=n_decoder_layers,\n",
    "#     sos_id = symbol_to_id[SOS_SYMBOL],\n",
    "#     teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "#     dropout=dropout\n",
    "#     )\n",
    "decoder = models.Decoder2(\n",
    "    n_symbols=n_symbols,\n",
    "    hidden_dim=hidden_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers=n_decoder_layers,\n",
    "    dropout=dropout\n",
    "    )\n",
    "model = models.EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:05<00:00, 58.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 109.332, val loss: 51.020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:05<00:00, 58.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 100.502, val loss: 45.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:05<00:00, 55.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss: 92.330, val loss: 41.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:05<00:00, 56.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train loss: 85.668, val loss: 38.077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:05<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train loss: 79.574, val loss: 35.499\n"
     ]
    }
   ],
   "source": [
    "# Training device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Training data\n",
    "train_dataset = datasets.WordDataset(\n",
    "    cur_train_sentences, text_to_id, n_symbols_max=n_symbols_max\n",
    "    )\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=datasets.pad_collate\n",
    "    )\n",
    "\n",
    "# Validation data\n",
    "val_dataset = datasets.WordDataset(cur_val_sentences, text_to_id)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=datasets.pad_collate\n",
    "    )\n",
    "\n",
    "# Loss\n",
    "criterion = nn.NLLLoss(\n",
    "    reduction=\"sum\", ignore_index=symbol_to_id[PAD_SYMBOL]\n",
    "    )\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i_epoch in range(n_epochs_max):\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(train_loader)):\n",
    "        optimiser.zero_grad()\n",
    "        data = data.to(device)       \n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_output.contiguous().view(-1, decoder_output.size(-1)),\n",
    "            data.contiguous().view(-1)\n",
    "            )\n",
    "        loss /= len(data_lengths)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (data, data_lengths) in enumerate(val_loader):\n",
    "            data = data.to(device)            \n",
    "            encoder_embedding, decoder_output = model(\n",
    "                data, data_lengths, data, data_lengths\n",
    "                )\n",
    "\n",
    "            loss = criterion(\n",
    "                decoder_output.contiguous().view(-1,\n",
    "                decoder_output.size(-1)), data.contiguous().view(-1)\n",
    "                )\n",
    "            loss /= len(data_lengths)\n",
    "            val_losses.append(loss.item())\n",
    "    \n",
    "    print(\n",
    "        \"Epoch {}, train loss: {:.3f}, val loss: {:.3f}\".format(\n",
    "        i_epoch,\n",
    "        np.mean(train_losses),\n",
    "        np.mean(val_losses))\n",
    "        )\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  68\n",
      "Output: 190_600_68_848_360_360_360_360_170_170_170_238_238_238_238_774_774_774_1021_236_1021_1021_1021_1021_1021\n",
      "\n",
      "Input:  178_314_100_367_552_220_38_586_236_516_792\n",
      "Output: 656_872_220_220_38_38_38_971_971_84_84_84_578_578_578_578_578_84_84_84_84_84_84_84_84\n",
      "\n",
      "Input:  106_300_38_932_16_432_172_930\n",
      "Output: 726_300_38_16_16_192_192_192_192_192_192_192_260_192_192_192_192_192_192_192_192_192_192_192_192\n",
      "\n",
      "Input:  618_666_192_772_1012_40_100_738_232_1021_6\n",
      "Output: 322_222_624_54_40_126_738_38_263_1021_84_84_24_578_578_578_578_578_578_578_578_578_578_578_578\n",
      "\n",
      "Input:  246_416_538_828_282_509\n",
      "Output: 314_752_600_282_282_282_818_818_372_372_220_220_220_220_220_220_220_220_220_220_220_220_220_220_220\n",
      "\n",
      "Input:  30_114_100_866\n",
      "Output: 738_404_332_332_98_100_100_100_100_100_100_100_100_100_100_100_100_100_100_100_100_100_100_100_100\n",
      "\n",
      "Input:  938_570_872_38_263\n",
      "Output: 570_372_476_60_38_263_263_263_263_192_192_263_263_263_263_263_263_263_263_263_263_263_263_263_263\n",
      "\n",
      "Input:  322_774_536_966_724_764_22_514_424_992\n",
      "Output: 314_536_876_764_764_764_724_832_424_588_992_994_994_994_994_994_994_994_994_994_994_994_994_1014_1014\n",
      "\n",
      "Input:  992_372_1010_828_282\n",
      "Output: 372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372_372\n",
      "\n",
      "Input:  836_770_172_616_812_68_112_68_950_66_362_698_652_482\n",
      "Output: 322_602_930_264_812_68_68_112_990_1002_330_330_76_652_652_652_652_652_652_652_652_652_652_652_652\n",
      "\n",
      "Input:  874_516_2_754_308_224_188_764_996_722_514_428_484_472_917\n",
      "Output: 42_618_410_308_308_224_644_644_722_722_873_588_714_484_484_484_484_484_484_484_484_484_484_484_484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples without segmentation\n",
    "\n",
    "# Apply to validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(val_loader):\n",
    "        data = data.to(device)\n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "        \n",
    "        y, log_probs = model.decoder.greedy_decode(\n",
    "            encoder_embedding,\n",
    "            max_length=25,\n",
    "            )\n",
    "        x = data.cpu().numpy()\n",
    "        \n",
    "        for i_input in range(y.shape[0]):\n",
    "            # Only print up to EOS symbol\n",
    "            input_symbols = []\n",
    "            for i in x[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                input_symbols.append(id_to_symbol[i])\n",
    "            output_symbols = []\n",
    "            for i in y[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                output_symbols.append(id_to_symbol[i])\n",
    "\n",
    "            print(\"Input: \", \"_\".join(input_symbols))\n",
    "            print(\"Output:\", \"_\".join(output_symbols))\n",
    "            print()\n",
    "            \n",
    "            if i_input == 10:\n",
    "                break\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utterances for evaluation\n",
    "n_eval_utterances = 1000 # 10000 # 1000\n",
    "# eval_sentences = prepared_text[-n_eval_utterances:]  # val sentences\n",
    "# eval_utterances = list(utterances)[-n_eval_utterances:]\n",
    "eval_sentences = prepared_text[:n_eval_utterances]\n",
    "eval_utterances = list(utterances)[:n_eval_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11831/11831 [00:48<00:00, 244.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Embed segments\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Data\n",
    "sentences = eval_sentences\n",
    "# sentences = cur_val_sentences\n",
    "interval_dataset = datasets.SentenceIntervalDataset(\n",
    "    sentences,\n",
    "    text_to_id,\n",
    "    join_char=\"_\"\n",
    "    )\n",
    "segment_loader = DataLoader(\n",
    "    interval_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=datasets.pad_collate,\n",
    "    drop_last=False\n",
    "    )\n",
    "\n",
    "# Apply model to data\n",
    "model.decoder.teacher_forcing_ratio = 1.0\n",
    "model.eval()\n",
    "rnn_losses = []\n",
    "lengths = []\n",
    "eos = []\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(segment_loader)):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        for i_item in range(data.shape[0]):\n",
    "            item_loss = criterion(\n",
    "                decoder_output[i_item].contiguous().view(-1,\n",
    "                decoder_output[i_item].size(-1)),\n",
    "                data[i_item].contiguous().view(-1)\n",
    "                )\n",
    "            rnn_losses.append(item_loss)\n",
    "            lengths.append(data_lengths[i_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:23<00:00, 41.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL: 115279.4234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Segment\n",
    "\n",
    "# dur_weight = 3.0 # Chorowski\n",
    "dur_weight = 1.0\n",
    "\n",
    "i_item = 0\n",
    "losses = []\n",
    "cur_segmented_sentences = []\n",
    "for i_sentence, intervals in enumerate(tqdm(interval_dataset.intervals)):\n",
    "    \n",
    "    # Costs for segment intervals\n",
    "    costs = np.inf*np.ones(len(intervals))\n",
    "    i_eos = intervals[-1][-1]\n",
    "    for i_seg, interval in enumerate(intervals):\n",
    "        if interval is None:\n",
    "            continue\n",
    "        i_start, i_end = interval\n",
    "        dur = i_end - i_start\n",
    "        assert dur == lengths[i_item]\n",
    "        eos = (i_end == i_eos)  # end-of-sequence\n",
    "        \n",
    "#         # Chorowski\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + dur_weight*neg_chorowski(dur)\n",
    "#             )\n",
    "        \n",
    "        # Gamma\n",
    "        costs[i_seg] = (\n",
    "            rnn_losses[i_item]\n",
    "            + dur_weight*neg_log_gamma(dur)\n",
    "            + np.log(np.sum(gamma_cache**dur_weight))\n",
    "            )\n",
    "        \n",
    "#         # Poisson\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + neg_log_poisson(dur)\n",
    "#             )\n",
    "\n",
    "#         # Histogram\n",
    "#         costs[i_seg] = (\n",
    "#             rnn_losses[i_item]\n",
    "#             + dur_weight*(neg_log_hist(dur))\n",
    "#             + np.log(np.sum(histogram**dur_weight))\n",
    "#             )\n",
    "    \n",
    "        # Sequence boundary\n",
    "        alpha = 0.9  # 0.3  # 0.9\n",
    "        if eos:\n",
    "            costs[i_seg] += -np.log(alpha)\n",
    "        else:\n",
    "            costs[i_seg] += -np.log(1 - alpha)\n",
    "\n",
    "        # Temp\n",
    "#         if dur > 10 or dur <= 1:\n",
    "#             costs[i_seg] = +np.inf\n",
    "        i_item += 1\n",
    "    \n",
    "    # Viterbi segmentation\n",
    "    n_frames = len(interval_dataset.sentences[i_sentence])\n",
    "    summed_cost, boundaries = viterbi.custom_viterbi(costs, n_frames)\n",
    "    losses.append(summed_cost)\n",
    "    \n",
    "    reference_sentence = sentences[i_sentence]\n",
    "    segmented_sentence = get_segmented_sentence(\n",
    "            interval_dataset.sentences[i_sentence],\n",
    "            boundaries\n",
    "            )\n",
    "    cur_segmented_sentences.append(segmented_sentence)\n",
    "#     # Print examples of the first few sentences\n",
    "#     if i_sentence < 10:\n",
    "#         print(reference_sentence)\n",
    "#         print(segmented_sentence)\n",
    "#         print()\n",
    "    \n",
    "print(\"NLL: {:.4f}\".format(np.sum(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598_930_86_812_68_263_512_658_652_772 484_502_474_672_120_332_736_234 1014_740_340_54_126 190_115_176_908\n"
     ]
    }
   ],
   "source": [
    "print(cur_segmented_sentences[0])\n",
    "\n",
    "# # To evaluate gold segmentation:\n",
    "# cur_segmented_sentences = prepared_text_gold[:n_eval_utterances]\n",
    "# print(cur_segmented_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 54046.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 22, '598_930_86_812_68_263_512_658_652_772'), (22, 44, '484_502_474_672_120_332_736_234'), (44, 56, '1014_740_340_54_126'), (56, 64, '190_115_176_908')]\n",
      "[(0, 19, 'those'), (19, 55, 'parents'), (55, 64, 'were')]\n",
      "\n",
      "[(0, 12, '228_648_498_492_316_534'), (12, 30, '714_680_748_716_960_509_100')]\n",
      "[(0, 11, 'i'), (11, 29, 'went')]\n",
      "\n",
      "[(0, 14, '854_116_752_1010_932_232_1021'), (14, 32, '154_716_924_28_776_758_156'), (32, 52, '268_136_430_855_995_960_848_364_524'), (52, 66, '874_546_74_988'), (66, 92, '298_474_672_367_552_776_252_920_660'), (92, 98, '244_285'), (98, 100, '416')]\n",
      "[(0, 20, 'no'), (20, 33, 'in'), (33, 57, 'one'), (57, 99, 'week')]\n",
      "\n",
      "[(0, 16, '46_34_712_180_448_988'), (16, 38, '314_100_866_68_508_804_930_856_726'), (38, 50, '518_980_378_990_210')]\n",
      "[(0, 9, 'when'), (9, 15, 'it'), (15, 42, 'happened'), (42, 49, 'in')]\n",
      "\n",
      "[(0, 14, '726_812_184_16_726_676'), (14, 28, '560_706_512_658_668_594'), (28, 46, '40_580_732_706_500_392_44'), (46, 68, '970_142_454_904_500_392_248_652_482_1002_388'), (68, 92, '262_58_928_71_458_995_96_644_636_28_776_360'), (92, 104, '313_168_64_804')]\n",
      "[(0, 10, 'it'), (10, 25, 'was'), (25, 59, 'useless'), (59, 69, 'to'), (69, 95, 'fight'), (95, 103, 'it')]\n",
      "\n",
      "[(0, 16, '982_30_180_364_666_518'), (16, 30, '608_876_68_628_380'), (30, 72, '772_1012_70_614_300_971_706_66_506_362_698_522')]\n",
      "[(0, 15, 'and'), (15, 31, 'not'), (31, 71, 'just')]\n",
      "\n",
      "[(0, 8, '272_968_54_160'), (8, 28, '940_498_188_786_810_58_396_484'), (28, 36, '1012_260_300_448'), (36, 62, '874_792_172_432_532_528_60_220_568_546'), (62, 78, '36_806_736_848_16_564'), (78, 102, '1012_288_204_802_164_788_240_765_648_498_644_316'), (102, 124, '786_362_122_76_482_772_482_962_222_118')]\n",
      "[(0, 24, 'tough'), (24, 37, 'to'), (37, 52, 'be'), (52, 67, 'able'), (67, 74, 'to'), (74, 123, 'trust')]\n",
      "\n",
      "[(0, 22, '246_62_342_248_362_122_820_48_72'), (22, 42, '724_764_258_316_401_148_588_992_666')]\n",
      "[(0, 40, 'son')]\n",
      "\n",
      "[(0, 30, '854_874_174_110_464_60_360_990_524_168_836'), (30, 54, '254_10_984_544_128_498_22_976_386_930'), (54, 66, '872_280_242_359_818_608'), (66, 84, '1010_538_114_244_484_952_54_130'), (84, 96, '14_38_990_836_666_518'), (96, 114, '752_854_170_740_624_340_663_838_210'), (114, 130, '476_528_300_38_263_68_418_22'), (130, 142, '712_971_144_12_176')]\n",
      "[(0, 12, 'gonna'), (12, 24, 'get'), (24, 28, 'a'), (28, 57, 'puppy'), (57, 66, 'and'), (66, 70, 'a'), (70, 100, 'kitten'), (100, 141, 'together')]\n",
      "\n",
      "[(0, 36, '42_646_570_528_60_38_120_509_112_903_508_536')]\n",
      "[(0, 35, 'yet')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert segmentation to intervals\n",
    "segmentation_interval_dict = {}\n",
    "for i_utt, utt_key in tqdm(enumerate(eval_utterances)):\n",
    "    words_segmented = cur_segmented_sentences[i_utt].split(\" \")\n",
    "    word_start = 0\n",
    "    word_label = \"\"\n",
    "    i_word = 0\n",
    "    segmentation_interval_dict[utt_key] = []\n",
    "    for (phone_start, phone_end,\n",
    "            phone_label) in phoneseg_interval_dict[utt_key]:\n",
    "        word_label += phone_label + \"_\"\n",
    "        if words_segmented[i_word] == word_label[:-1]:\n",
    "            segmentation_interval_dict[utt_key].append((\n",
    "                word_start, phone_end, word_label[:-1]\n",
    "                ))\n",
    "            word_label = \"\"\n",
    "            word_start = phone_end\n",
    "            i_word += 1\n",
    "\n",
    "    if i_utt < 10:\n",
    "        print(segmentation_interval_dict[utt_key])\n",
    "        print(word_ref_interval_dict[utt_key])\n",
    "        print()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 271282.84it/s]\n",
      "100%|██████████| 16582/16582 [00:00<00:00, 306015.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Word boundaries:\n",
      "Precision: 30.92%\n",
      "Recall: 32.86%\n",
      "F-score: 31.86%\n",
      "OS: 6.25%\n",
      "R-value: 40.33%\n",
      "---------------------------------------------------------------------------\n",
      "Word token boundaries:\n",
      "Precision: 17.33%\n",
      "Recall: 22.03%\n",
      "F-score: 19.40%\n",
      "OS: 27.12%\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Intervals to boundaries\n",
    "segmentation_boundaries_dict = {}\n",
    "for utt_key in tqdm(segmentation_interval_dict):\n",
    "    segmentation_boundaries_dict[utt_key] = eval_segmentation.intervals_to_boundaries(\n",
    "        segmentation_interval_dict[utt_key]\n",
    "        )\n",
    "word_ref_boundaries_dict = {}\n",
    "for utt_key in tqdm(word_ref_interval_dict):\n",
    "    word_ref_boundaries_dict[utt_key] = eval_segmentation.intervals_to_boundaries(\n",
    "        word_ref_interval_dict[utt_key]\n",
    "        )\n",
    "\n",
    "# Evaluate word boundaries\n",
    "reference_list = []\n",
    "segmentation_list = []\n",
    "for utterance in segmentation_boundaries_dict:\n",
    "    reference_list.append(word_ref_boundaries_dict[utterance])\n",
    "    segmentation_list.append(segmentation_boundaries_dict[utterance])\n",
    "\n",
    "tolerance = 2\n",
    "p, r, f = eval_segmentation.score_boundaries(\n",
    "    reference_list, segmentation_list, tolerance=tolerance\n",
    "    )\n",
    "print(\"-\"*(79 - 4))\n",
    "print(\"Word boundaries:\")\n",
    "print(\"Precision: {:.2f}%\".format(p*100))\n",
    "print(\"Recall: {:.2f}%\".format(r*100))\n",
    "print(\"F-score: {:.2f}%\".format(f*100))\n",
    "print(\"OS: {:.2f}%\".format(eval_segmentation.get_os(p, r)*100))\n",
    "print(\"R-value: {:.2f}%\".format(eval_segmentation.get_rvalue(p, r)*100))\n",
    "print(\"-\"*(79 - 4))\n",
    "\n",
    "p, r, f = eval_segmentation.score_word_token_boundaries(\n",
    "    reference_list, segmentation_list, tolerance=tolerance\n",
    "    )\n",
    "print(\"Word token boundaries:\")\n",
    "print(\"Precision: {:.2f}%\".format(p*100))\n",
    "print(\"Recall: {:.2f}%\".format(r*100))\n",
    "print(\"F-score: {:.2f}%\".format(f*100))\n",
    "print(\"OS: {:.2f}%\".format(eval_segmentation.get_os(p, r)*100))\n",
    "# print(\"R-value: {:.2f}%\".format(get_rvalue(p, r)*100))\n",
    "print(\"-\"*(79 - 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering_sentences = prepared_text_gold[:10000]\n",
    "# clustering_sentences = prepared_text[:10000]  # probably doesn't make sense\n",
    "clustering_sentences = cur_segmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 494.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3973, 50)\n",
      "2021-08-02 09:49:30.916564\n",
      "Clustering: K = 4096\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_samples=3973 should be >= n_clusters=4096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5ee735ed575e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clustering: K = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mvq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mvq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inertia: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mcentroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n\u001b[0m\u001b[1;32m    998\u001b[0m                 _num_samples(X), self.n_clusters))\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=3973 should be >= n_clusters=4096"
     ]
    }
   ],
   "source": [
    "# K-means centroids\n",
    "\n",
    "# Data\n",
    "train_dataset = datasets.WordDataset(\n",
    "    clustering_sentences, text_to_id, n_symbols_max=n_symbols_max\n",
    "    )\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=datasets.pad_collate\n",
    "    )\n",
    "\n",
    "# Apply model to data\n",
    "model.eval()\n",
    "encoder_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(train_loader)):\n",
    "        data = data.to(device)\n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "        encoder_embeddings.append(encoder_embedding.cpu().numpy())\n",
    "        \n",
    "# Cluster\n",
    "X = np.vstack(encoder_embeddings)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(datetime.now())\n",
    "K = 4096 # 1024  # 1024  # 2048\n",
    "print(\"Clustering: K = {}\".format(K))\n",
    "vq_model = cluster.KMeans(n_clusters=K, max_iter=10)\n",
    "vq_model.fit(X)\n",
    "print(\"Inertia: {:.4f}\".format(vq_model.inertia_))\n",
    "centroids = vq_model.cluster_centers_\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples without segmentation\n",
    "\n",
    "# Apply to validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(val_loader):\n",
    "#     for i_batch, (data, data_lengths) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        encoder_embedding = encoder_embedding.cpu().numpy()\n",
    "        clusters = vq_model.predict(encoder_embedding)\n",
    "        embedding_reconstructed = centroids[clusters, :].reshape(\n",
    "            encoder_embedding.shape\n",
    "            )\n",
    "        embedding_reconstructed = torch.from_numpy(\n",
    "            embedding_reconstructed\n",
    "            ).to(device)\n",
    "        \n",
    "        y, log_probs = model.decoder.greedy_decode(\n",
    "            embedding_reconstructed,\n",
    "            max_length=n_symbols_max,\n",
    "            )\n",
    "        x = data.cpu().numpy()\n",
    "        \n",
    "        for i_input in range(y.shape[0]):\n",
    "            # Only print up to EOS symbol\n",
    "            input_symbols = []\n",
    "            for i in x[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                input_symbols.append(id_to_symbol[i])\n",
    "            output_symbols = []\n",
    "            for i in y[i_input]:\n",
    "                if i == symbol_to_id[EOS_SYMBOL] or i == symbol_to_id[PAD_SYMBOL]:\n",
    "                    break\n",
    "                output_symbols.append(id_to_symbol[i])\n",
    "\n",
    "            print(\"Input: \", \"_\".join(input_symbols))\n",
    "            print(\"Output:\", \"_\".join(output_symbols))\n",
    "            print()\n",
    "            \n",
    "            if i_input == 10:\n",
    "                break\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utterances for evaluation\n",
    "n_eval_utterances = 1000\n",
    "# eval_sentences = prepared_text[-n_eval_utterances:]  # val sentences\n",
    "# eval_utterances = list(utterances)[-n_eval_utterances:]\n",
    "eval_sentences = prepared_text[:n_eval_utterances]\n",
    "eval_utterances = list(utterances)[:n_eval_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed segments\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Data\n",
    "sentences = eval_sentences\n",
    "interval_dataset = datasets.SentenceIntervalDataset(\n",
    "    sentences,\n",
    "    text_to_id,\n",
    "    \"_\"\n",
    "    )\n",
    "segment_loader = DataLoader(\n",
    "    interval_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=datasets.pad_collate,\n",
    "    drop_last=False\n",
    "    )\n",
    "\n",
    "# Apply model to data\n",
    "model.decoder.teacher_forcing_ratio = 1.0  # to-do: adjust this\n",
    "model.eval()\n",
    "rnn_losses = []\n",
    "lengths = []\n",
    "with torch.no_grad():\n",
    "    for i_batch, (data, data_lengths) in enumerate(tqdm(segment_loader)):\n",
    "        data = data.to(device)\n",
    "\n",
    "        encoder_embedding, decoder_output = model(\n",
    "            data, data_lengths, data, data_lengths\n",
    "            )\n",
    "\n",
    "        encoder_embedding = encoder_embedding.cpu().numpy()\n",
    "        clusters = vq_model.predict(encoder_embedding)\n",
    "        embedding_reconstructed = centroids[clusters, :].reshape(\n",
    "            encoder_embedding.shape\n",
    "            )\n",
    "        embedding_reconstructed = torch.from_numpy(\n",
    "            embedding_reconstructed\n",
    "            ).to(device)\n",
    "        \n",
    "        decoder_rnn, decoder_output = model.decoder(\n",
    "            embedding_reconstructed, data, data_lengths\n",
    "            )\n",
    "\n",
    "        for i_item in range(data.shape[0]):\n",
    "            item_loss = criterion(\n",
    "                decoder_output[i_item].contiguous().view(-1,\n",
    "                decoder_output[i_item].size(-1)),\n",
    "                data[i_item].contiguous().view(-1)\n",
    "                )\n",
    "            rnn_losses.append(item_loss)\n",
    "            lengths.append(data_lengths[i_item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:\n",
    "\n",
    "- Want to evaluate this segmentation: Go back up to the cell where segmentation is done (after segments are embedded).\n",
    "- Want to retrain K-means model based on this segmentation: Go back to start of quantization cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
